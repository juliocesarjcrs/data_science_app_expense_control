{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcdb2211-73e7-4a7f-af07-25ef7acc7891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-17 13:55:02.756940: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-17 13:55:02.835676: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-17 13:55:03.189998: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-17 13:55:03.192101: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-17 13:55:04.339355: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import sys\n",
    "sys.path.append('/code/src/')\n",
    "from my_functions import train_test_time_series, evaluate_forecast, load_dataframe_from_csv, split_train_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3dabe8d-d989-46e3-9c7c-e4fc8da125bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type= <class 'list'>\n",
      "MinMaxScaler()\n",
      "[ 1. 18.]\n",
      "a MinMaxScaler()\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
    "scaler = MinMaxScaler()\n",
    "print('type=', type(data))\n",
    "a = scaler.fit(data)\n",
    "print(scaler.fit(data))\n",
    "print(scaler.data_max_)\n",
    "print('a', a)\n",
    "# NFEATS= 3\n",
    "# scalers = [MinMaxScaler(feature_range=(-1,1)) for i in range(NFEATS)] \n",
    "# scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "354591b7-1202-48c3-ad2b-faee1327bb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño set de entrenamiento: (35,)\n",
      "Tamaño set de prueba: (9,)\n",
      "Tamaños entrada (BATCHES x INPUT_LENGTH x FEATURES) y de salida (BATCHES x OUTPUT_LENGTH x FEATURES)\n",
      "Set de entrenamiento - x_tr: (32, 3, 1), y_tr: (32, 1, 1)\n",
      "Set de prueba - x_ts: (6, 3, 1), y_ts: (6, 1, 1)\n",
      "Min x_tr/x_ts sin escalamiento: 645200/2505194\n",
      "Min x_tr/x_ts con escalamiento: -1.3074767890295889/0.44819263096457684\n",
      "\n",
      "Min y_tr/y_ts sin escalamiento: 645200/2505194\n",
      "Min y_tr/y_ts con escalamiento: -1.305147127618661/0.37938306465445154\n",
      "\n",
      "Max x_tr/x_ts sin escalamiento: 4345208/5296946\n",
      "Max x_tr/x_ts con escalamiento: 2.330258246350595/3.556685010756135\n",
      "\n",
      "Max y_tr/y_ts sin escalamiento: 4345208/4466767\n",
      "Max y_tr/y_ts con escalamiento: 2.0458180839100075/2.155909732400555\n",
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-17 15:21:16.073136: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-06-17 15:21:16.074265: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-06-17 15:21:16.075093: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-06-17 15:21:16.146245: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_10' with dtype float and shape [32,3,1]\n",
      "\t [[{{node Placeholder/_10}}]]\n",
      "2023-06-17 15:21:16.146500: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_11' with dtype float and shape [32,1,1]\n",
      "\t [[{{node Placeholder/_11}}]]\n",
      "2023-06-17 15:21:16.326579: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-06-17 15:21:16.327633: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-06-17 15:21:16.328764: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-06-17 15:21:16.778175: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-06-17 15:21:16.779895: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-06-17 15:21:16.780918: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 1s - loss: 1.0429 - 1s/epoch - 1s/step\n",
      "Epoch 2/80\n",
      "1/1 - 0s - loss: 1.0395 - 4ms/epoch - 4ms/step\n",
      "Epoch 3/80\n",
      "1/1 - 0s - loss: 1.0370 - 6ms/epoch - 6ms/step\n",
      "Epoch 4/80\n",
      "1/1 - 0s - loss: 1.0350 - 7ms/epoch - 7ms/step\n",
      "Epoch 5/80\n",
      "1/1 - 0s - loss: 1.0331 - 4ms/epoch - 4ms/step\n",
      "Epoch 6/80\n",
      "1/1 - 0s - loss: 1.0314 - 5ms/epoch - 5ms/step\n",
      "Epoch 7/80\n",
      "1/1 - 0s - loss: 1.0298 - 6ms/epoch - 6ms/step\n",
      "Epoch 8/80\n",
      "1/1 - 0s - loss: 1.0283 - 14ms/epoch - 14ms/step\n",
      "Epoch 9/80\n",
      "1/1 - 0s - loss: 1.0269 - 8ms/epoch - 8ms/step\n",
      "Epoch 10/80\n",
      "1/1 - 0s - loss: 1.0255 - 4ms/epoch - 4ms/step\n",
      "Epoch 11/80\n",
      "1/1 - 0s - loss: 1.0242 - 5ms/epoch - 5ms/step\n",
      "Epoch 12/80\n",
      "1/1 - 0s - loss: 1.0229 - 6ms/epoch - 6ms/step\n",
      "Epoch 13/80\n",
      "1/1 - 0s - loss: 1.0216 - 5ms/epoch - 5ms/step\n",
      "Epoch 14/80\n",
      "1/1 - 0s - loss: 1.0204 - 5ms/epoch - 5ms/step\n",
      "Epoch 15/80\n",
      "1/1 - 0s - loss: 1.0192 - 5ms/epoch - 5ms/step\n",
      "Epoch 16/80\n",
      "1/1 - 0s - loss: 1.0180 - 5ms/epoch - 5ms/step\n",
      "Epoch 17/80\n",
      "1/1 - 0s - loss: 1.0168 - 6ms/epoch - 6ms/step\n",
      "Epoch 18/80\n",
      "1/1 - 0s - loss: 1.0156 - 6ms/epoch - 6ms/step\n",
      "Epoch 19/80\n",
      "1/1 - 0s - loss: 1.0145 - 5ms/epoch - 5ms/step\n",
      "Epoch 20/80\n",
      "1/1 - 0s - loss: 1.0133 - 5ms/epoch - 5ms/step\n",
      "Epoch 21/80\n",
      "1/1 - 0s - loss: 1.0122 - 5ms/epoch - 5ms/step\n",
      "Epoch 22/80\n",
      "1/1 - 0s - loss: 1.0111 - 6ms/epoch - 6ms/step\n",
      "Epoch 23/80\n",
      "1/1 - 0s - loss: 1.0099 - 4ms/epoch - 4ms/step\n",
      "Epoch 24/80\n",
      "1/1 - 0s - loss: 1.0088 - 15ms/epoch - 15ms/step\n",
      "Epoch 25/80\n",
      "1/1 - 0s - loss: 1.0077 - 7ms/epoch - 7ms/step\n",
      "Epoch 26/80\n",
      "1/1 - 0s - loss: 1.0066 - 6ms/epoch - 6ms/step\n",
      "Epoch 27/80\n",
      "1/1 - 0s - loss: 1.0055 - 7ms/epoch - 7ms/step\n",
      "Epoch 28/80\n",
      "1/1 - 0s - loss: 1.0045 - 4ms/epoch - 4ms/step\n",
      "Epoch 29/80\n",
      "1/1 - 0s - loss: 1.0034 - 4ms/epoch - 4ms/step\n",
      "Epoch 30/80\n",
      "1/1 - 0s - loss: 1.0023 - 5ms/epoch - 5ms/step\n",
      "Epoch 31/80\n",
      "1/1 - 0s - loss: 1.0012 - 5ms/epoch - 5ms/step\n",
      "Epoch 32/80\n",
      "1/1 - 0s - loss: 1.0002 - 5ms/epoch - 5ms/step\n",
      "Epoch 33/80\n",
      "1/1 - 0s - loss: 0.9991 - 4ms/epoch - 4ms/step\n",
      "Epoch 34/80\n",
      "1/1 - 0s - loss: 0.9980 - 4ms/epoch - 4ms/step\n",
      "Epoch 35/80\n",
      "1/1 - 0s - loss: 0.9970 - 4ms/epoch - 4ms/step\n",
      "Epoch 36/80\n",
      "1/1 - 0s - loss: 0.9959 - 5ms/epoch - 5ms/step\n",
      "Epoch 37/80\n",
      "1/1 - 0s - loss: 0.9948 - 4ms/epoch - 4ms/step\n",
      "Epoch 38/80\n",
      "1/1 - 0s - loss: 0.9938 - 4ms/epoch - 4ms/step\n",
      "Epoch 39/80\n",
      "1/1 - 0s - loss: 0.9927 - 4ms/epoch - 4ms/step\n",
      "Epoch 40/80\n",
      "1/1 - 0s - loss: 0.9917 - 4ms/epoch - 4ms/step\n",
      "Epoch 41/80\n",
      "1/1 - 0s - loss: 0.9906 - 6ms/epoch - 6ms/step\n",
      "Epoch 42/80\n",
      "1/1 - 0s - loss: 0.9896 - 5ms/epoch - 5ms/step\n",
      "Epoch 43/80\n",
      "1/1 - 0s - loss: 0.9886 - 5ms/epoch - 5ms/step\n",
      "Epoch 44/80\n",
      "1/1 - 0s - loss: 0.9875 - 4ms/epoch - 4ms/step\n",
      "Epoch 45/80\n",
      "1/1 - 0s - loss: 0.9865 - 4ms/epoch - 4ms/step\n",
      "Epoch 46/80\n",
      "1/1 - 0s - loss: 0.9854 - 4ms/epoch - 4ms/step\n",
      "Epoch 47/80\n",
      "1/1 - 0s - loss: 0.9844 - 4ms/epoch - 4ms/step\n",
      "Epoch 48/80\n",
      "1/1 - 0s - loss: 0.9834 - 4ms/epoch - 4ms/step\n",
      "Epoch 49/80\n",
      "1/1 - 0s - loss: 0.9823 - 7ms/epoch - 7ms/step\n",
      "Epoch 50/80\n",
      "1/1 - 0s - loss: 0.9813 - 4ms/epoch - 4ms/step\n",
      "Epoch 51/80\n",
      "1/1 - 0s - loss: 0.9803 - 4ms/epoch - 4ms/step\n",
      "Epoch 52/80\n",
      "1/1 - 0s - loss: 0.9793 - 6ms/epoch - 6ms/step\n",
      "Epoch 53/80\n",
      "1/1 - 0s - loss: 0.9782 - 4ms/epoch - 4ms/step\n",
      "Epoch 54/80\n",
      "1/1 - 0s - loss: 0.9772 - 5ms/epoch - 5ms/step\n",
      "Epoch 55/80\n",
      "1/1 - 0s - loss: 0.9762 - 4ms/epoch - 4ms/step\n",
      "Epoch 56/80\n",
      "1/1 - 0s - loss: 0.9752 - 4ms/epoch - 4ms/step\n",
      "Epoch 57/80\n",
      "1/1 - 0s - loss: 0.9741 - 6ms/epoch - 6ms/step\n",
      "Epoch 58/80\n",
      "1/1 - 0s - loss: 0.9731 - 8ms/epoch - 8ms/step\n",
      "Epoch 59/80\n",
      "1/1 - 0s - loss: 0.9721 - 6ms/epoch - 6ms/step\n",
      "Epoch 60/80\n",
      "1/1 - 0s - loss: 0.9711 - 4ms/epoch - 4ms/step\n",
      "Epoch 61/80\n",
      "1/1 - 0s - loss: 0.9701 - 5ms/epoch - 5ms/step\n",
      "Epoch 62/80\n",
      "1/1 - 0s - loss: 0.9690 - 7ms/epoch - 7ms/step\n",
      "Epoch 63/80\n",
      "1/1 - 0s - loss: 0.9680 - 7ms/epoch - 7ms/step\n",
      "Epoch 64/80\n",
      "1/1 - 0s - loss: 0.9670 - 4ms/epoch - 4ms/step\n",
      "Epoch 65/80\n",
      "1/1 - 0s - loss: 0.9660 - 5ms/epoch - 5ms/step\n",
      "Epoch 66/80\n",
      "1/1 - 0s - loss: 0.9650 - 7ms/epoch - 7ms/step\n",
      "Epoch 67/80\n",
      "1/1 - 0s - loss: 0.9640 - 4ms/epoch - 4ms/step\n",
      "Epoch 68/80\n",
      "1/1 - 0s - loss: 0.9630 - 6ms/epoch - 6ms/step\n",
      "Epoch 69/80\n",
      "1/1 - 0s - loss: 0.9620 - 4ms/epoch - 4ms/step\n",
      "Epoch 70/80\n",
      "1/1 - 0s - loss: 0.9610 - 4ms/epoch - 4ms/step\n",
      "Epoch 71/80\n",
      "1/1 - 0s - loss: 0.9600 - 5ms/epoch - 5ms/step\n",
      "Epoch 72/80\n",
      "1/1 - 0s - loss: 0.9590 - 5ms/epoch - 5ms/step\n",
      "Epoch 73/80\n",
      "1/1 - 0s - loss: 0.9579 - 4ms/epoch - 4ms/step\n",
      "Epoch 74/80\n",
      "1/1 - 0s - loss: 0.9569 - 4ms/epoch - 4ms/step\n",
      "Epoch 75/80\n",
      "1/1 - 0s - loss: 0.9559 - 5ms/epoch - 5ms/step\n",
      "Epoch 76/80\n",
      "1/1 - 0s - loss: 0.9549 - 6ms/epoch - 6ms/step\n",
      "Epoch 77/80\n",
      "1/1 - 0s - loss: 0.9539 - 4ms/epoch - 4ms/step\n",
      "Epoch 78/80\n",
      "1/1 - 0s - loss: 0.9529 - 4ms/epoch - 4ms/step\n",
      "Epoch 79/80\n",
      "1/1 - 0s - loss: 0.9520 - 4ms/epoch - 4ms/step\n",
      "Epoch 80/80\n",
      "1/1 - 0s - loss: 0.9510 - 4ms/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-17 15:21:18.188192: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_10' with dtype float and shape [32,3,1]\n",
      "\t [[{{node Placeholder/_10}}]]\n",
      "2023-06-17 15:21:18.188445: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_11' with dtype float and shape [32,1,1]\n",
      "\t [[{{node Placeholder/_11}}]]\n",
      "2023-06-17 15:21:18.376883: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-06-17 15:21:18.377890: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-06-17 15:21:18.378870: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-06-17 15:21:18.538139: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_11' with dtype float and shape [6,1,1]\n",
      "\t [[{{node Placeholder/_11}}]]\n",
      "2023-06-17 15:21:18.538372: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_11' with dtype float and shape [6,1,1]\n",
      "\t [[{{node Placeholder/_11}}]]\n",
      "2023-06-17 15:21:18.708149: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-06-17 15:21:18.709608: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-06-17 15:21:18.711551: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparativo desempeños:\n",
      "  RMSE train:\t 0.950\n",
      "  RMSE test:\t 1.122\n"
     ]
    }
   ],
   "source": [
    "#mio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import tensorflow as tf\n",
    "from IPython.display import display\n",
    "import sys\n",
    "sys.path.append('/code/src/')\n",
    "from my_functions import train_test_time_series, evaluate_forecast, load_dataframe_from_csv, split_train_test_data\n",
    "def create_sequences(data, sequence_length, output_length):\n",
    "    X, Y = [], []\n",
    "    shape = data.shape\n",
    "    if len(shape)==1: # Si tenemos sólo una serie (univariado)\n",
    "        rows, cols = data.shape[0], 1\n",
    "        data = data.reshape(rows,cols)\n",
    "    else:\n",
    "        rows, cols = data.shape\n",
    "    # Generar los arreglo\n",
    "    for i in range(rows- sequence_length):\n",
    "        X.append(data[i: i + sequence_length, 0: cols])\n",
    "        Y.append(data[i + sequence_length : i + sequence_length + output_length, -1].reshape(output_length,1))\n",
    "    # Convertir listas a arreglos de NumPy\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    return X, Y\n",
    "def train_test_split(data):\n",
    "    train_size = int(len(data) * 0.8)\n",
    "    train_data = data[:train_size]\n",
    "    test_data = data[train_size:]\n",
    "    return train_data, test_data\n",
    "def escalar_dataset(data_input):\n",
    "    NFEATS = data_input['x_tr'].shape[2]\n",
    "\n",
    "    # Generar listado con \"scalers\"\n",
    "    # scalers = [MinMaxScaler(feature_range=(0,1)) for i in range(NFEATS)] #feature_rangetuple (min, max), default=(0, 1)\n",
    "    scalers = [StandardScaler() for i in range(NFEATS)] #feature_rangetuple (min, max), default=(0, 1)\n",
    "\n",
    "    # Arreglos que contendrán los datasets escalados\n",
    "    x_tr_s = np.zeros(data_input['x_tr'].shape)\n",
    "    # x_vl_s = np.zeros(data_input['x_vl'].shape)\n",
    "    x_ts_s = np.zeros(data_input['x_ts'].shape)\n",
    "    y_tr_s = np.zeros(data_input['y_tr'].shape)\n",
    "    # y_vl_s = np.zeros(data_input['y_vl'].shape)\n",
    "    y_ts_s = np.zeros(data_input['y_ts'].shape)\n",
    "\n",
    "    # Escalamiento: se usarán los min/max del set de entrenamiento para\n",
    "    # escalar la totalidad de los datasets\n",
    "    \n",
    "    # Escalamiento Xs\n",
    "    for i in range(NFEATS):\n",
    "        x_tr_s[:,:,i] = scalers[i].fit_transform(data_input['x_tr'][:,:,i])\n",
    "        # x_vl_s[:,:,i] = scalers[i].transform(x_vl[:,:,i])\n",
    "        x_ts_s[:,:,i] = scalers[i].transform(data_input['x_ts'][:,:,i])\n",
    "    \n",
    "    # Escalamiento Ys\n",
    "    y_tr_s[:,:,0] = scalers[-1].fit_transform(data_input['y_tr'][:,:,0])\n",
    "    # y_vl_s[:,:,0] = scalers[-1].transform(y_vl[:,:,0])\n",
    "    y_ts_s[:,:,0] = scalers[-1].transform(data_input['y_ts'][:,:,0])\n",
    "\n",
    "    # Conformar ` de salida\n",
    "    data_scaled = {\n",
    "        'x_tr_s': x_tr_s, 'y_tr_s': y_tr_s,\n",
    "        # 'x_vl_s': x_vl_s, 'y_vl_s': y_vl_s,\n",
    "        'x_ts_s': x_ts_s, 'y_ts_s': y_ts_s,\n",
    "    }\n",
    "\n",
    "    return data_scaled, scalers[0]\n",
    "\n",
    "def main():\n",
    "    path_load = \"../../data/\"\n",
    "    file_name = \"processed/df_time_monthly.csv\"\n",
    "    full_path = path_load + file_name\n",
    "    data = load_dataframe_from_csv(full_path)\n",
    "    data.index.freq = 'M'\n",
    "    #\n",
    "    INPUT_LENGTH = 3\n",
    "    OUTPUT_LENGTH = 1\n",
    "    train_data, test_data =train_test_split(data['cost'])\n",
    "    # Imprimir en pantalla el tamaño de cada subset\n",
    "    print(f'Tamaño set de entrenamiento: {train_data.shape}')\n",
    "    print(f'Tamaño set de prueba: {test_data.shape}')\n",
    "\n",
    "    x_tr, y_tr = create_sequences(train_data.values, INPUT_LENGTH, OUTPUT_LENGTH)\n",
    "    x_ts, y_ts = create_sequences(test_data.values, INPUT_LENGTH, OUTPUT_LENGTH)\n",
    "    print('Tamaños entrada (BATCHES x INPUT_LENGTH x FEATURES) y de salida (BATCHES x OUTPUT_LENGTH x FEATURES)')\n",
    "    print(f'Set de entrenamiento - x_tr: {x_tr.shape}, y_tr: {y_tr.shape}')\n",
    "    print(f'Set de prueba - x_ts: {x_ts.shape}, y_ts: {y_ts.shape}')\n",
    "\n",
    "    # Preprocesamiento de los datos\n",
    "    data_in = {\n",
    "    'x_tr': x_tr, 'y_tr': y_tr,\n",
    "    # 'x_vl': x_vl, 'y_vl': y_vl,\n",
    "    'x_ts': x_ts, 'y_ts': y_ts,\n",
    "    }\n",
    "    data_s, scaler = escalar_dataset(data_in)\n",
    "    # Extraer subsets escalados\n",
    "    x_tr_s, y_tr_s = data_s['x_tr_s'], data_s['y_tr_s']\n",
    "    x_ts_s, y_ts_s = data_s['x_ts_s'], data_s['y_ts_s']\n",
    "    \n",
    "    # Verificación\n",
    "    print(f'Min x_tr/x_ts sin escalamiento: {x_tr.min()}/{x_ts.min()}')\n",
    "    print(f'Min x_tr/x_ts con escalamiento: {x_tr_s.min()}/{x_ts_s.min()}')\n",
    "    \n",
    "    print(f'\\nMin y_tr/y_ts sin escalamiento: {y_tr.min()}/{y_ts.min()}')\n",
    "    print(f'Min y_tr/y_ts con escalamiento: {y_tr_s.min()}/{y_ts_s.min()}')\n",
    "    \n",
    "    print(f'\\nMax x_tr/x_ts sin escalamiento: {x_tr.max()}/{x_ts.max()}')\n",
    "    print(f'Max x_tr/x_ts con escalamiento: {x_tr_s.max()}/{x_ts_s.max()}')\n",
    "    \n",
    "    print(f'\\nMax y_tr/y_ts sin escalamiento: {y_tr.max()}/{y_ts.max()}')\n",
    "    print(f'Max y_tr/y_ts con escalamiento: {y_tr_s.max()}/{y_ts_s.max()}')\n",
    "\n",
    "    # Ajustar parámetros para reproducibilidad del entrenamiento\n",
    "    tf.random.set_seed(123)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "    \n",
    "    # El modelo\n",
    "    N_UNITS = 64 # Tamaño del estado oculto (h) y de la celda de memoria (c)\n",
    "    INPUT_SHAPE = (x_tr_s.shape[1], x_tr_s.shape[2]) # 3 (meses) x 1 (feature)\n",
    "    \n",
    "    modelo = Sequential()\n",
    "    modelo.add(LSTM(N_UNITS, input_shape=INPUT_SHAPE))\n",
    "    modelo.add(Dense(OUTPUT_LENGTH, activation='linear'))\n",
    "\n",
    "    # Compilación\n",
    "    optimizador = RMSprop(learning_rate=5e-5)\n",
    "    modelo.compile(\n",
    "        optimizer = optimizador,\n",
    "        loss=\"mean_squared_error\",\n",
    "    )\n",
    "    \n",
    "    # Entrenamiento (aproximadamente 1 min usando GPU)\n",
    "    EPOCHS = 80 # Hiperparámetro\n",
    "    BATCH_SIZE = 256 # Hiperparámetro\n",
    "    historia = modelo.fit(\n",
    "        x = x_tr_s,\n",
    "        y = y_tr_s,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        epochs = EPOCHS,\n",
    "        # validation_data = (x_vl_s, y_vl_s),\n",
    "        verbose=2\n",
    "    )\n",
    "    rmse_tr = modelo.evaluate(x=x_tr_s, y=y_tr_s, verbose=0)\n",
    "    # rmse_vl = modelo.evaluate(x=x_vl_s, y=y_vl_s, verbose=0)\n",
    "    rmse_ts = modelo.evaluate(x=x_ts_s, y=y_ts_s, verbose=0)\n",
    "\n",
    "    # Imprimir resultados en pantalla\n",
    "    print('Comparativo desempeños:')\n",
    "    print(f'  RMSE train:\\t {rmse_tr:.3f}')\n",
    "    # print(f'  RMSE val:\\t {rmse_vl:.3f}')\n",
    "    print(f'  RMSE test:\\t {rmse_ts:.3f}')\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3011e18a-5454-46aa-9222-aef6b7bfee8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cf9c586-8b94-45ab-9580-8d98d42dd1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# def main():\n",
    "#     path_load = \"../../data/\"\n",
    "#     file_name = \"processed/df_time_monthly.csv\"\n",
    "#     full_path = path_load + file_name\n",
    "#     data = load_dataframe_from_csv(full_path)\n",
    "#     data.index.freq = 'M'\n",
    "#     print(data.head(5))\n",
    "    \n",
    "#     train_size = int(len(data) * 0.8)\n",
    "#     train_data = data[:train_size]\n",
    "#     test_data = data[train_size:]\n",
    "#     print('--- train_data: ', train_data.shape)\n",
    "#     print('--- test_data: ', test_data.shape)\n",
    "    \n",
    "#     # Preprocesamiento de los datos\n",
    "#     scaler = MinMaxScaler()\n",
    "#     train_scaled = scaler.fit_transform(train_data[[\"cost\"]])\n",
    "    \n",
    "#     # Crear secuencias de entrenamiento\n",
    "#     sequence_length = 3  # Longitud de la secuencia\n",
    "#     X_train, y_train = create_sequences(train_scaled, sequence_length)\n",
    "    \n",
    "#     # Crear y entrenar el modelo LSTM\n",
    "#     model = create_lstm_model(sequence_length)\n",
    "#     model.fit(X_train, y_train, epochs=100, verbose=0)\n",
    "    \n",
    "#     # Preparar los datos de prueba\n",
    "#     test_scaled = scaler.transform(test_data[[\"cost\"]])\n",
    "#     X_test, y_test = create_sequences(test_scaled, sequence_length)\n",
    "    \n",
    "#     # Realizar predicciones\n",
    "#     predictions = model.predict(X_test)\n",
    "    \n",
    "#     # Invertir la escala de las predicciones\n",
    "#     predictions = scaler.inverse_transform(predictions)\n",
    "    \n",
    "#     # Crear un DataFrame con las predicciones y las fechas correspondientes\n",
    "#     prediction_dates = test_data['date'][sequence_length:]\n",
    "\n",
    "#     predictions_df = pd.DataFrame(predictions, index=prediction_dates, columns=[\"predicted_cost\"])\n",
    "#     display(predictions_df)\n",
    "\n",
    "#     # predictions = []\n",
    "#     # print('len(X_test)', len(X_test))\n",
    "#     # for i in range(len(X_test)):\n",
    "#     #     prediction = model.predict(np.array([X_test[i]]))\n",
    "#     #     predictions = scaler.inverse_transform(predictions)\n",
    "#     #     predictions.append(prediction[0])\n",
    "    \n",
    "#     # prediction_dates = test_data['date'][sequence_length:]\n",
    "#     # predictions_df = pd.DataFrame(predictions, index=prediction_dates, columns=[\"predicted_cost\"])\n",
    "#     # display(predictions_df)\n",
    "\n",
    "# def load_dataframe_from_csv(file_path):\n",
    "#     return pd.read_csv(file_path)\n",
    "\n",
    "# def create_sequences(data, sequence_length):\n",
    "#     X = []\n",
    "#     y = []\n",
    "#     for i in range(len(data) - sequence_length):\n",
    "#         X.append(data[i:i+sequence_length])\n",
    "#         y.append(data[i+sequence_length])\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# def create_lstm_model(sequence_length):\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(64, input_shape=(sequence_length, 1)))\n",
    "#     model.add(Dense(1))\n",
    "#     model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "#     return model\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2c759459-ff34-442e-ac2d-b986620b1ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_sequences(data, sequence_length):\n",
    "#     X = []\n",
    "#     y = []\n",
    "#     for i in range(len(data) - sequence_length):\n",
    "#         X.append(data.iloc[i:i+sequence_length].values)\n",
    "#         y.append(data.iloc[i+sequence_length][\"cost\"])\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# def main():\n",
    "#     path_load = \"../../data/\"\n",
    "#     file_name = \"processed/df_time_monthly.csv\"\n",
    "#     full_path = path_load + file_name\n",
    "#     data = load_dataframe_from_csv(full_path)\n",
    "#     data.index.freq = 'M'\n",
    "#     display(data.head(5))\n",
    "#     print(type(data))\n",
    "    \n",
    "#     train_size = int(len(data) * 0.8)\n",
    "#     train_data = data[:train_size]\n",
    "#     test_data = data[train_size:]\n",
    "#     print('--- train_data: ', train_data.shape)\n",
    "#     print('--- test_data: ', test_data.shape)\n",
    "#     raise SystemExit\n",
    "#     scaler = MinMaxScaler()\n",
    "#     train_scaled = scaler.fit_transform(train_data)\n",
    "#     test_scaled = scaler.transform(test_data)\n",
    "    \n",
    "#     train_scaled = pd.DataFrame(train_scaled, columns=data.columns)\n",
    "#     test_scaled = pd.DataFrame(test_scaled, columns=data.columns)\n",
    "\n",
    "#     sequence_length = 5\n",
    "#     X_train, y_train = create_sequences(train_scaled, sequence_length)\n",
    "#     X_test, y_test = create_sequences(test_scaled, sequence_length)\n",
    "#     print(X_train.shape)\n",
    "#     print(y_train.shape)\n",
    "\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(64, activation='relu', input_shape=(sequence_length, train_scaled.shape[1])))\n",
    "#     model.add(Dense(1))\n",
    "#     model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "#     model.fit(X_train, y_train, epochs=10, batch_size=16)\n",
    "\n",
    "#     train_loss = model.evaluate(X_train, y_train, verbose=0)\n",
    "#     test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "#     print(f'Train Loss: {train_loss:.4f}')\n",
    "#     print(f'Test Loss: {test_loss:.4f}')\n",
    "\n",
    "#     # Paso 8: Realizar predicciones\n",
    "#     predictions = model.predict(X_test)\n",
    "#     predictions = np.reshape(predictions, (-1, 1))  # Ajustar la forma de las predicciones\n",
    "#     predictions = scaler.inverse_transform(predictions)  # Invertir la escala utilizando el mismo escalador\n",
    "    \n",
    "#     # Crear un DataFrame con las predicciones\n",
    "#     predictions_df = pd.DataFrame(predictions, columns=[\"predicted_cost\"])\n",
    "#     predictions_df.index = test_data.index[sequence_length:]  # Ajustar el índice para que coincida con las fechas correspondientes\n",
    "    \n",
    "#     # Imprimir las predicciones\n",
    "#     print(predictions_df)\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cf7aa3f8-164b-4ca9-bff0-44f8a92044a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_sequences(data, sequence_length):\n",
    "#     X = []\n",
    "#     y = []\n",
    "#     for i in range(len(data) - sequence_length):\n",
    "#         X.append(data.iloc[i:i+sequence_length].values)\n",
    "#         y.append(data.iloc[i+sequence_length][\"cost\"])\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# def main():\n",
    "#     path_load = \"../../data/\"\n",
    "#     file_name = \"processed/df_time_monthly.csv\"\n",
    "#     full_path = path_load + file_name\n",
    "#     data = load_dataframe_from_csv(full_path)\n",
    "#     data.index.freq = 'M'\n",
    "#     # train_X, train_y, test_X, test_y = split_train_test_data(data, 'cost')\n",
    "    \n",
    "#     # Paso 2: Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "#     train_size = int(len(data) * 0.8)  # El 80% de los datos se utiliza para entrenamiento\n",
    "#     train_data = data[:train_size]\n",
    "#     test_data = data[train_size:]\n",
    "#     print('--- train_data: ', train_data.shape)\n",
    "#     print('--- test_data: ', test_data.shape)\n",
    "#     # Paso 3: Preprocesamiento de datos\n",
    "#     scaler = MinMaxScaler()\n",
    "#     train_scaled = scaler.fit_transform(train_data)\n",
    "#     test_scaled = scaler.transform(test_data)\n",
    "#     # print('---type test_data: ', type(test_data), test_data.shape)\n",
    "#     # print('---type test_scaled: ', type(test_scaled), test_scaled.shape)\n",
    "#     # display(test_data.head(2))\n",
    "#     # display(test_scaled[:2])\n",
    "#     # Convertir las matrices NumPy en DataFrames\n",
    "#     train_scaled = pd.DataFrame(train_scaled, columns=data.columns)\n",
    "#     test_scaled = pd.DataFrame(test_scaled, columns=data.columns)\n",
    "\n",
    "#     # Paso 4: Crear secuencias de entrada y salida\n",
    "#     sequence_length = 5  # Longitud de la secuencia de entrada\n",
    "#     X_train, y_train = create_sequences(train_scaled, sequence_length)\n",
    "#     X_test, y_test = create_sequences(test_scaled, sequence_length)\n",
    "#     print(X_train.shape)\n",
    "#     print(y_train.shape)\n",
    "\n",
    "#     # Paso 5: Construir el modelo LSTM\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(64, activation='relu', input_shape=(sequence_length, train_scaled.shape[1])))\n",
    "#     model.add(Dense(1))\n",
    "#     model.compile(optimizer='adam', loss='mse')\n",
    "#     # Paso 6: Entrenar el modelo\n",
    "#     model.fit(X_train, y_train, epochs=10, batch_size=16)\n",
    "#     # Paso 7: Evaluar el modelo\n",
    "#     train_loss = model.evaluate(X_train, y_train, verbose=0)\n",
    "#     test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "#     print(f'Train Loss: {train_loss:.4f}')\n",
    "#     print(f'Test Loss: {test_loss:.4f}')\n",
    "    \n",
    "#     # Paso 8: Realizar predicciones\n",
    "#     # Paso 8: Realizar predicciones\n",
    "#     predictions = model.predict(X_test)\n",
    "#     scaler_predictions = MinMaxScaler()\n",
    "#     scaler_predictions.fit(data)  # Ajustar el escalador a los datos originales sin escalar\n",
    "#     predictions = np.reshape(predictions, (-1, train_scaled.shape[1]))  # Ajustar la forma de las predicciones\n",
    "#     predictions = scaler_predictions.inverse_transform(predictions)  # Invertir la escala\n",
    "    \n",
    "#     print('---train_scaled', train_scaled.shape)\n",
    "#     print('-----X_test', X_test.shape)\n",
    "#     print('---predictions.shape', predictions.shape)\n",
    "#     print('predictions:', predictions)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d910942c-f601-4d96-a948-88d06633a986",
   "metadata": {},
   "source": [
    "### Creando un modelo de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78ff38e9-c343-471b-aa3d-a4420bd16371",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512,activation='relu', input_shape=(28*28,)))\n",
    "model.add(layers.Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0b56ef2-50b3-426f-8606-c5a2271f40bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570307f2-541d-4d42-abc5-7cf6a18e6b7e",
   "metadata": {},
   "source": [
    "## Limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1776417a-561d-4524-bfed-3337e43dc656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data.reshape((60000,28*28))\n",
    "x_train = x_train.astype('float32')/255\n",
    "\n",
    "x_test = test_data.reshape((10000,28*28))\n",
    "x_test = x_test.astype('float32')/255\n",
    "y_train = to_categorical(train_labels)\n",
    "y_test = to_categorical(test_labels)\n",
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80c7491-6546-4090-86b8-6a8e847ac527",
   "metadata": {},
   "source": [
    "## Entrenando el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d6e3e96-5983-45d1-9e93-19a4a12115e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.2611 - accuracy: 0.9247\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1072 - accuracy: 0.9686\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0707 - accuracy: 0.9790\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0512 - accuracy: 0.9844\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0378 - accuracy: 0.9887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f77e057dfd0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5, batch_size=128) # demora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57eb647-90f9-464a-906a-be53fc391919",
   "metadata": {},
   "source": [
    "## Evaluando sobre data de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d38b54fe-f413-46ef-882c-1cfd8d95c6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 1ms/step - loss: 0.0700 - accuracy: 0.9789\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07004410028457642, 0.9789000153541565]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
